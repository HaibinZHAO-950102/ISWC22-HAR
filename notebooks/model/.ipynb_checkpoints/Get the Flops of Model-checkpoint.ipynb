{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef766dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Normal convolution block\n",
    "    \"\"\"\n",
    "    def __init__(self, filter_width, input_filters, nb_filters, dilation, batch_norm):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.filter_width = filter_width\n",
    "        self.input_filters = input_filters\n",
    "        self.nb_filters = nb_filters\n",
    "        self.dilation = dilation\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.input_filters, self.nb_filters, (self.filter_width, 1), dilation=(self.dilation, 1))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(self.nb_filters, self.nb_filters, (self.filter_width, 1), dilation=(self.dilation, 1), stride=(2,1))\n",
    "        if self.batch_norm:\n",
    "            self.norm1 = nn.BatchNorm2d(self.nb_filters)\n",
    "            self.norm2 = nn.BatchNorm2d(self.nb_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        if self.batch_norm:\n",
    "            out = self.norm1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        if self.batch_norm:\n",
    "            out = self.norm2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MCNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_shape, \n",
    "                 nb_classes,\n",
    "                 filter_scaling_factor=1,\n",
    "                 ):\n",
    "                 #nb_conv_blocks        = 2,\n",
    "                 #nb_filters            = 64,\n",
    "                 #dilation              = 1,\n",
    "                 #batch_norm            = False,\n",
    "                 #filter_width          = 5,\n",
    "                 #nb_layers_lstm        = 2,\n",
    "                 #drop_prob             = 0.5,\n",
    "                 #nb_units_lstm         = 128):\n",
    "        \"\"\"\n",
    "        DeepConvLSTM model based on architecture suggested by Ordonez and Roggen (https://www.mdpi.com/1424-8220/16/1/115)\n",
    "        \n",
    "        \"\"\"\n",
    "        super(MCNN, self).__init__()\n",
    "        self.nb_conv_blocks = 2\n",
    "        self.nb_filters     = 32\n",
    "        self.dilation       = 1\n",
    "        self.batch_norm     = True\n",
    "        self.filter_width   = 5\n",
    "        self.drop_prob      = 0.1\n",
    "\n",
    "        \n",
    "        \n",
    "        self.nb_channels = input_shape[3]\n",
    "        self.nb_classes = nb_classes\n",
    "\n",
    "    \n",
    "        self.conv_blocks = []\n",
    "\n",
    "        for i in range(self.nb_conv_blocks):\n",
    "            if i == 0:\n",
    "                input_filters = input_shape[1]\n",
    "            else:\n",
    "                input_filters = self.nb_filters\n",
    "    \n",
    "            self.conv_blocks.append(ConvBlock(self.filter_width, input_filters, self.nb_filters, self.dilation, self.batch_norm))\n",
    "\n",
    "        \n",
    "        self.conv_blocks = nn.ModuleList(self.conv_blocks)\n",
    "        shape = self.get_the_shape(input_shape)\n",
    "        final_length  = shape[2]\n",
    "        # B F L* C\n",
    "\n",
    "        # define dropout layer\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        \n",
    "        # Sensor Fusion\n",
    "        self.activation = nn.ReLU() \n",
    "        self.fc_sensor_fusion = nn.Linear(self.nb_filters*self.nb_channels ,2*self.nb_filters)\n",
    "        \n",
    "        # Temporal Fusion   \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_temporal_fusion = nn.Linear(2*self.nb_filters*final_length ,self.nb_filters*2)\n",
    "        \n",
    "        # define classifier\n",
    "        self.fc_prediction = nn.Linear(self.nb_filters*2, self.nb_classes)\n",
    "\n",
    "\n",
    "    def get_the_shape(self, input_shape):\n",
    "        x = torch.rand(input_shape)\n",
    "        for conv_block in self.conv_blocks:\n",
    "            x = conv_block(x)    \n",
    "\n",
    "        return x.shape\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        for i, conv_block in enumerate(self.conv_blocks):\n",
    "            x = conv_block(x)\n",
    "        # B F L* C\n",
    "\n",
    "    \n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        # B L*  F C\n",
    "\n",
    "        x = x.reshape(x.shape[0], x.shape[1], self.nb_filters * self.nb_channels)\n",
    "        x = self.dropout(x)\n",
    "        # B L*  F*C\n",
    "\n",
    "        x = self.activation(self.fc_sensor_fusion(x)) \n",
    "        # B L*  2*C\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.fc_temporal_fusion(x)) # B L C\n",
    "\n",
    "\n",
    "        out = self.fc_prediction(x)    \n",
    "\n",
    "        return out\n",
    "\n",
    "    def number_of_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4400a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MCNN((1,1,128,6),6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf4026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# ====================================  model ==================================== #\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Normal convolution block\n",
    "    \"\"\"\n",
    "    def __init__(self, filter_width, input_filters, nb_filters, dilation, batch_norm):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.filter_width = filter_width\n",
    "        self.input_filters = input_filters\n",
    "        self.nb_filters = nb_filters\n",
    "        self.dilation = dilation\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.input_filters, self.nb_filters, (self.filter_width, 1), dilation=(self.dilation, 1),padding='same')\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(self.nb_filters, 1, (self.filter_width, 1), dilation=(self.dilation, 1), stride=(1,1),padding='same')\n",
    "        if self.batch_norm:\n",
    "            self.norm1 = nn.BatchNorm2d(self.nb_filters)\n",
    "            self.norm2 = nn.BatchNorm2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        if self.batch_norm:\n",
    "            out = self.norm1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        if self.batch_norm:\n",
    "            out = self.norm2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class SensorAttention(nn.Module):\n",
    "    def __init__(self, input_shape, nb_filters ):\n",
    "        super(SensorAttention, self).__init__()\n",
    "        self.ln = nn.LayerNorm(input_shape[3])        #  channel的维度\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=nb_filters, kernel_size=3, dilation=2, padding='same')\n",
    "        self.conv_f = nn.Conv2d(in_channels=nb_filters, out_channels=1, kernel_size=1, padding='same')\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=3)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        input: [batch  * length * channel]\n",
    "        output: [batch, 1, length, d]\n",
    "        '''\n",
    "        # layer norm 在最后一个维度，  原文是在feature上\n",
    "        inputs = self.ln(inputs)               \n",
    "        # 增加 维度， tensorflow 是在最后一个维度上加， torch是第一个\n",
    "        x = inputs.unsqueeze(1)                \n",
    "        # b 1 L C\n",
    "        x = self.conv_1(x)              \n",
    "        x = self.relu(x)  \n",
    "        # b 128 L C\n",
    "        x = self.conv_f(x)               \n",
    "        # b 1 L C\n",
    "        x = self.softmax(x)\n",
    "        x = x.squeeze(1)                  # batch * channel * len \n",
    "        # B L C\n",
    "        return torch.mul(inputs, x), x    # batch * channel * len, batch * channel * len \n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "\n",
    "        self.query_projection = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.key_projection   = nn.Linear(d_model, d_model, bias=True)\n",
    "        self.value_projection = nn.Linear(d_model, d_model, bias=True)\n",
    "        self.out_projection   = nn.Linear(d_model, d_model, bias=True)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        B, L, _ = queries.shape\n",
    "\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, L, H, -1)\n",
    "        values = self.value_projection(values).view(B, L, H, -1)\n",
    "\n",
    "\n",
    "\n",
    "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
    "        _, _, _, E = queries.shape\n",
    "        scale = 1./math.sqrt(E)\n",
    "        Attn = torch.softmax(scale * scores, dim=-1)\n",
    "        V = torch.einsum(\"bhls,bshd->blhd\", Attn, values).contiguous()\n",
    "\n",
    "        out = V.view(B, L, -1)\n",
    "        out = self.out_projection(out)\n",
    "        return out, Attn\n",
    "    \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,  d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "\n",
    "\n",
    "        self.attention = AttentionLayer(d_model, n_heads)\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)    \n",
    "        \n",
    "        \n",
    "        d_ff = d_ff or 4*d_model\n",
    "        self.ffn1 = nn.Linear(d_model, d_ff, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ffn2 = nn.Linear(d_ff, d_model, bias=True)\n",
    "\n",
    "         \n",
    "        self.layernorm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)               \n",
    "        \n",
    "\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        attn_output, attn = self.attention( x, x, x )\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1  = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn2(self.relu(self.ffn1(out1)))\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 =  self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n",
    "\n",
    "class AttentionWithContext(nn.Module):\n",
    "    def __init__(self, token_d_model):\n",
    "        super(AttentionWithContext, self).__init__()\n",
    "        self.W = nn.Linear(token_d_model, token_d_model)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.u = nn.Linear(token_d_model, 1, bias=False)\n",
    "    def forward(self, inputs):\n",
    "        uit = self.W(inputs)\n",
    "        uit = self.tanh(uit)\n",
    "        ait = self.u(uit)\n",
    "        outputs = torch.matmul(F.softmax(ait, dim=1).transpose(-1, -2),inputs).squeeze(-2)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class SA_HAR(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_shape, \n",
    "                 nb_classes, \n",
    "                 filter_scaling_factor, \n",
    "                 config):\n",
    "        super(SA_HAR, self).__init__()\n",
    "\n",
    "        self.nb_filters     = 32\n",
    "\n",
    "        self.first_conv = ConvBlock(filter_width=5, \n",
    "                                    input_filters=input_shape[1], \n",
    "                                    nb_filters=self.nb_filters, \n",
    "                                    dilation=1, \n",
    "                                    batch_norm=True).double()\n",
    "        \n",
    "        self.SensorAttention = SensorAttention(input_shape,self.nb_filters)\n",
    "        self.conv1d = nn.Conv1d(in_channels=input_shape[3], out_channels=self.nb_filters, kernel_size=1)\n",
    "        \n",
    "        \n",
    "        #self.pos_embedding = nn.Parameter(self.sinusoidal_embedding(input_shape[2], self.nb_filters), requires_grad=False)\n",
    "        #self.pos_dropout = nn.Dropout(p=0.2) \n",
    "        \n",
    "        self.EncoderLayer1 = EncoderLayer( d_model = self.nb_filters, n_heads =4 , d_ff = self.nb_filters*4)\n",
    "        self.EncoderLayer2 = EncoderLayer( d_model = self.nb_filters, n_heads =4 , d_ff = self.nb_filters*4)\n",
    "\n",
    "\n",
    "        self.AttentionWithContext = AttentionWithContext(self.nb_filters)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.nb_filters, 4*nb_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.fc_out = nn.Linear(4*nb_classes, nb_classes)      # 从d_dim到6classes, 取72是论文中说的4倍classes数（4*18）\n",
    "\n",
    "    \n",
    "    def forward(self,x): \n",
    "        # x -- > B  fin  length Chennel\n",
    "        x = self.first_conv(x)\n",
    "        x = x.squeeze(1) \n",
    "        # x -- > B length Chennel\n",
    "\t\n",
    "        # B L C\n",
    "        si, _ = self.SensorAttention(x) \n",
    "        \n",
    "        # B L C\n",
    "        x = self.conv1d(si.permute(0,2,1)).permute(0,2,1) \n",
    "        x = self.relu(x)            \n",
    "        # B L C\n",
    "        #x = x + self.pos_embedding\n",
    "        #x = self.pos_dropout(x)\n",
    "\n",
    "        x = self.EncoderLayer1(x)            # batch * len * d_dim\n",
    "        x = self.EncoderLayer2(x)            # batch * len * d_dim\n",
    "        \n",
    "        # Global Temporal Attention\n",
    "        x = self.AttentionWithContext(x)\n",
    "\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc_out(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def sinusoidal_embedding(length, dim):\n",
    "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
    "                                for p in range(length)])\n",
    "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
    "        return pe.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9e730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e818492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9514098b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55246706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0e797fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135206"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.number_of_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e48113fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "from ptflops import get_model_complexity_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dd6f86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: variables __flops__ or __params__ are already defined for the moduleConv2d ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleReLU ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleConv2d ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleBatchNorm2d ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleBatchNorm2d ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleConv2d ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleReLU ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleConv2d ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleBatchNorm2d ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleBatchNorm2d ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleReLU ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleLinear ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleLinear ptflops can affect your code!\n",
      "Warning: variables __flops__ or __params__ are already defined for the moduleLinear ptflops can affect your code!\n",
      "Computational complexity:       5.11 MMac\n",
      "Number of parameters:           135.21 k\n"
     ]
    }
   ],
   "source": [
    "\n",
    "macs, params = get_model_complexity_info(model, (1,128,6), as_strings=True,\n",
    "                                       print_per_layer_stat=False, verbose=False)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d2cc2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.11 MMac'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5ffd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
